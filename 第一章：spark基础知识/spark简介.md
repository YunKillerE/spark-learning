# Spark主要源自于MapReduce，但是MapReduce有一些局限性

 首先，要实现复杂的功能，要做很多的优化工作。大家从名字上也可以看出来，MapReduce的优点是写起程序来非常简单，缺点是太简单了，以至于写复杂的程序要做大量的工作。

 第二个是性能，十年前磁盘是比较廉价的，内存是比较昂贵的，那时候很多的观念都是基于磁盘来优化设计的，而Mapreduce就是这样做的，很多的设计理念是基于磁盘进行的。所以导致他的性能非常低效。然而，今天已经发生了变化了，磁盘成为了过去时，Spark就在这块做了一些尝试，想尽可能的在内存上做一些工作，优化框架。


# 当时另外的一个背景是多种框架并存

研究MapReduce的人在设计框架的时候，发现了多种框架是并存的。当时一共有三种并行计算的场景：

第一种是批处理，也是最典型的场景，比如说我要产生一个报表，明天看到就可以了，处理方式是MapReduce/Hive；

第二个是交互式处理，最典型的是以Impala和Presto为代表的数据库查询引擎；

第三种是实时处理，典型的是storm。比如阿里巴巴有个滚动的大屏幕，要实时的统计每天截至凌晨1点的交易量是多少，有一笔记录，就做一个汇总，就是实时的统计，实时的汇总交易数据，并做出汇总展示。这就是当时多种框架并存的局面。

如果你们公司每一种应用场景都需要的话，你可能至少要有三套系统，有三个集群，这样的话管理运行起来非常的麻烦。是不是能设计一套系统解决这三个场景，而且他们是一套，这样的话学习起来都是统一的，接口都是统一的，维护起来也是统一的，而不是搭建三套，每一套都需要单独学习，因为它由不同的生态构成的。这样的话，你需要三个系统，这三个系统是完全的不同的方式，你的学习成本和运维成本都会非常高。

那么我们做了一个尝试，Spark的特点，一是非常的高效，统计上看，比MapReduce快10到100倍，从数据上可以看到，MapReduce是非常慢的框架。快多少倍取决于具体的场景。

# 为什么这么高效呢？

第一，它是DAG的引擎，而MapReduce也可以把它看成DAG的引擎，但是非常简单的，它只有两个，点和边。反过来，如果想做非常复杂的运算，需要非常大量的作业。用MapReduce做的话这个要做四个作业，是通过存储不同的MapReduce作业衔接在一起的。而通过DAG引擎，所有的数据都不需要落在这种文件系统里，直接可以通过流式的，或者是其它的方式衔接起来。这是Spark的一个核心的设计理念。

第二，它可以充分的使用内存。SparK提供了充分使用内存的能力，可以把数据处理完之后落到磁盘上或者是其它地方，是非常灵活的。实际上，如果你不想用内存，可以关掉，这个时候可以做一些简单的运算。另外是易用的，Spark提供了四种编程语言，代码量也比MapReduce要小2到5倍。现在有一个趋势，简单的编程语言越来越受欢迎，比如说Scala和Python。

第三个是与Hadoop的集成，Spark可以读写HDFS/Hbase，并且与YARN集成。

Spark核心理念，一个是RDD，弹性分布式数据集，对分布式数据抽象，你可以采用其它方式来实现，也是没有问题的。在RDD上，如果把这个数据转成RDD后，它提供了丰富的编程接口。比说有map，filter等等，有多少个结点都可以扩展。

Spark还有个非常强大的方式，就是部署方式非常的灵活，你可以运行在本地，也可以运行在standalone上，也可以部署在mesos/yarn上。

# Spark是一个生态系统

很多人不会编程，你可以用Spark SQL处理，你只要会写SQL就OK了；

你有图的数据，做图计算，可以使用GraphX；

如果你想做距离的分类，推荐等等，有一个Mllib，这样的话，你直接利用这里提供的就OK了。

Spark是一个生态系统，是一个软件站，不同的软件会解决不同的问题，比如说是不是我只用SQL就行了，不好意思，有一些机器学习算法不可能只用SQL来表达，大数据，包括人工智能，这些机器学习、深度学习技术已经越来越流行。这块是SQL解决不了的，很多必须要用APR来写，当然更多的是做一些实时统计，这种可以Spark SQL。随着大数据的发展，很多很多的数据处理都逐渐的流式化，所以streaming也越来越受欢迎。


来源：董西成|Spark2.0新特性与展望